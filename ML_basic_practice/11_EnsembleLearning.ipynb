{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11_EnsembleLearning.ipynb","provenance":[],"authorship_tag":"ABX9TyMF6+74p+hRKq6BZDqoS65w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**개요 :** 앙상블 학습에 대해서 배웠다. 결정 트리 기반의 앙상블 학습은 강력하고 뛰어난 성능을 제공하기 때문에 인기가 많다. 사이킷런에서 제공하는 앙상블 학습 알고리즘 중 랜덤 포레스트, 엑스트라 트리, 그레이디언트 부스팅, 히스토그램 기반 그레이디언트 부스팅을 다루었다.   \n","**랜덤 포레스트**는 가장 대표적인 앙상블 학습 알고리즘이다. 성능이 좋고 안정적이기 때문에 첫 번재고 시도해 볼 수 있는 앙상ㅂ즐 학습 중 하나이다. 랜덤 포레스트는 결정 트리를 훈련하기 위해 부트스트랩 샘플을 만들고 전체 특성 중 일부를 랜덤하게 선택하여 결정 트리를 만든다.   \n","**엑스트라 트리**는 랜덤 포레스트와 매우 비슷하지만 부트스트랩 샘플을 사용하지 않고 노드를 분할할때 최선이 아니라 랜덤하게 분할한다. 이런 특징 때문에 랜덤 포레스트보다 훈련 속도가 빠르지만 보통 더 많은 트리가 필요하다.   \n","**그레이디언트 부스팅**은 깊이가 얕은 트리를 연속적으로 추가하여 손실 함수를 최소화하는 앙상블 방법이다. 성능이 뛰어나지만 병렬로 훈련할 수 없기 때문에 랜덤 포레스트나 엑스트라 트리보다 훈련 속도가 조금 느리다. 그레이디언트 부스팅에서 학습률 매개변수를 조정하여 모델의 복잡도를 제어할 수 있다. 학습률 매개변수가 크면 복잡하고 훈련 세트에 과대적합된 모델을 얻을 수 있다.   \n","**히스토그램 기반 그레이디언트 부스팅** 알고리즘은 가장 뛰어난 앙상블 학습으로 평가받는다. 훈련 데이터를 256개의 구간으로 변환하여 사용하기 때문에 노드 분할 속도가 매우 빠르다."],"metadata":{"id":"Z1UMY-7Yl5uN"}},{"cell_type":"markdown","source":["### **정형 데이터와 비정형 데이터**\n","**정형 데이터(structured data) :** CSV 파일이나 데이터베이스, 엑셀에 저장하기 쉬운 데이터   \n","**비정형 데이터(unstructed data) :** 데이터베이스나 엑셀로 표현하기 어려운 것들. 예를 들어 책의 글과 같은 텍스트 데이터, 디지털카메라로 찍은 사진, 핸드폰으로 듣는 디지털 음악 등이 있다."],"metadata":{"id":"K8fviaHk_ihb"}},{"cell_type":"markdown","source":["### **앙상블 학습(Ensemble Learining)**\n","정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘. 이 알고리즘은 대부분 결정 트리를 기반으로 만들어져 있다. 더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는 머신러닝 알고리즘을 말한다."],"metadata":{"id":"dU-TSAT6Awj8"}},{"cell_type":"markdown","source":["### **랜덤 포레스트(Random Forest)**\n","랜덤 포레스트는 앙상블 학습의 대표 주자 중 하나로 안정적인 성능 덕분에 널리 사용되고 있다.   \n","랜덤 포레스트는 결정 트리를 랜덤하게 만들어 결정 트리(나무)의 숲을 만든다. 그리고 각 결정 트리의 예측을 사용해 최종 예측을 만든다.   \n","랜덤 포레스트는 훈련 세트에서 중복을 허용하여 부트스트랩 샘플을 만들어 결정 트리를 훈현한다."],"metadata":{"id":"Ss5_zcCLBDiY"}},{"cell_type":"markdown","source":["### **부트스트랩 샘플(bootstrap sample)**\n","중복을 허용하도록 뽑은 샘플"],"metadata":{"id":"NZqYdAWsCeSX"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","wine = pd.read_csv('https://bit.ly/wine_csv_data')\n","data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n","target = wine['class'].to_numpy()\n","train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)"],"metadata":{"id":"JbK38bqxDpJ7","executionInfo":{"status":"ok","timestamp":1658317377133,"user_tz":-540,"elapsed":287,"user":{"displayName":"박민규","userId":"03566772411550627054"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import cross_validate\n","from sklearn.ensemble import RandomForestClassifier\n","rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n","scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PrSOP4EPEqPJ","executionInfo":{"status":"ok","timestamp":1658317383393,"user_tz":-540,"elapsed":5836,"user":{"displayName":"박민규","userId":"03566772411550627054"}},"outputId":"b37078bd-f4ef-47da-f485-8a6d147bb25b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9973541965122431 0.8905151032797809\n"]}]},{"cell_type":"code","source":["rf.fit(train_input, train_target)\n","print(rf.feature_importances_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8iVeCZNLFzeb","executionInfo":{"status":"ok","timestamp":1658317384436,"user_tz":-540,"elapsed":1047,"user":{"displayName":"박민규","userId":"03566772411550627054"}},"outputId":"943e9c5e-54a7-428c-b4ce-7c6651fa1439"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.23167441 0.50039841 0.26792718]\n"]}]},{"cell_type":"markdown","source":["부트스트랩 샘플에 포함되지 않고 남는 샘플을 **OOB(out of bag)** 라고 한다. 이 남는 샘플을 사용하여 부트스트랩 샘플로 훈련한 결정 트리를 평가할 수 있다."],"metadata":{"id":"ByhhKXAmH37j"}},{"cell_type":"code","source":["rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n","rf.fit(train_input, train_target)\n","print(rf.oob_score_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7HFLMtrJGyjI","executionInfo":{"status":"ok","timestamp":1658317385901,"user_tz":-540,"elapsed":1468,"user":{"displayName":"박민규","userId":"03566772411550627054"}},"outputId":"123885d2-f1b8-4e42-d01b-37892cc8f491"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8934000384837406\n"]}]},{"cell_type":"markdown","source":["### **엑스트라 트리(Extra Tree)**\n","엑스트라 트리는 랜덤 포레스트와 매우 비슷하게 동작한다. 랜덤 포레스트와 엑스트라 트리의 차이점은 부트스트랩 샘플을 사용하지 않는다는 점이다. 즉 각 결정 트리를 만들 때 전체 훈련 세트를 사용한다. 대신 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할한다.   \n","하나의 결정 트리에서 특성을 무작위로 분할한다면 성능이 낮아지겠지만 많은 트리를 앙상블 하기 때문에 과대적합을 막고 검증 세트의 점수를 높이는 효과가 있다. "],"metadata":{"id":"7azPvgzxIXnX"}},{"cell_type":"code","source":["from sklearn.ensemble import ExtraTreesClassifier\n","et = ExtraTreesClassifier()\n","scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RGUifwWxHu3z","executionInfo":{"status":"ok","timestamp":1658317624352,"user_tz":-540,"elapsed":2947,"user":{"displayName":"박민규","userId":"03566772411550627054"}},"outputId":"75df9646-2d8c-4efd-86fd-d8cfb32a1c1d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9974503966084433 0.8878229806766861\n"]}]},{"cell_type":"markdown","source":["랜덤 포레스트와 비슷한 결과를 얻었다. 이 예제는 특성이 많지 않아 두 모델의 차이가 크지 않다. 보통 엑스트라 트리의 무작위성이 좀 더 크기 때문에 랜덤 포레스트보다 더 많은 결정트리를 훈련해야 한다. 하지만 랜덤하게 노드를 분할하기 때문에 빠른 계산 속도가 엑스트라 트리의 장점이다."],"metadata":{"id":"HWZIKyQ4bvcT"}},{"cell_type":"code","source":["et.fit(train_input, train_target)\n","print(et.feature_importances_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CvwQog8XbsZ9","executionInfo":{"status":"ok","timestamp":1658317730870,"user_tz":-540,"elapsed":723,"user":{"displayName":"박민규","userId":"03566772411550627054"}},"outputId":"08320c74-2423-42cc-9a19-a301631b3ead"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.19518085 0.52045035 0.2843688 ]\n"]}]},{"cell_type":"markdown","source":["### **그레이디언트 부스팅(Gradient Boosting)**\n","그레이디언트 부스팅은 깊이가 얕은 결정트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법이다. 깊이가 얕은 결정 트리를 사용하기 때문에 과대적합에 강하고 일반적으로 높은 일반화 성능을 기대할 수 있다.   \n","경사 하강법을 사용하여 트리를 앙상블에 추가한다. 분류에서는 로지스틱 손실 함수를 사용하고 회귀에서는 평균 제곱 오차 함수를 사용한다."],"metadata":{"id":"uhdZu0ZucMpr"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingClassifier\n","gb = GradientBoostingClassifier(random_state=42)\n","scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SL0jNvezcG_B","executionInfo":{"status":"ok","timestamp":1658318035829,"user_tz":-540,"elapsed":4666,"user":{"displayName":"박민규","userId":"03566772411550627054"}},"outputId":"e415f65f-6793-48e8-a095-cf837b8b3dec"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8881086892152563 0.8720430147331015\n"]}]},{"cell_type":"code","source":["gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n","scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WYXnw5CGdLcB","executionInfo":{"status":"ok","timestamp":1658318257904,"user_tz":-540,"elapsed":10824,"user":{"displayName":"박민규","userId":"03566772411550627054"}},"outputId":"bf3b687b-61e9-4ba9-8a1f-cb444e6b3872"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9464595437171814 0.8780082549788999\n"]}]},{"cell_type":"markdown","source":["결정 트리의 개수를 500개로 5배나 늘렸지만 과대적합을 잘 억제하고 있다. 학습률 learning_rate의 기본값은 0.1 이다."],"metadata":{"id":"pgA4Nn8FeF2y"}},{"cell_type":"code","source":["gb.fit(train_input, train_target)\n","print(gb.feature_importances_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I8dOgqHVd_We","executionInfo":{"status":"ok","timestamp":1658318326987,"user_tz":-540,"elapsed":3345,"user":{"displayName":"박민규","userId":"03566772411550627054"}},"outputId":"0844b90d-ccf2-4ac1-fcee-597e45e758eb"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.15872278 0.68010884 0.16116839]\n"]}]},{"cell_type":"markdown","source":["### **히스토그램 기반 그레이디언트 부스팅(Histogram-based Gradient Boosting)**\n","히스토그램 기반 그레이디언트 부스팅은 정형 데이터를 다루는 머신러닝 알고리즘 중에 가장 인기가 높은 알고리즘이다. 히스토그램 기반 그레이디언트 부스팅은 먼저 입력 특성을 256개 구간으로 나눈다. 따라서 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있다. 히스토그램 기반 그레이디언트 부스팅은 256개의 구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용한다. 따라서 입력에 누락된 특성이 있더라도 이를 따로 전처리할 필요가 없다."],"metadata":{"id":"V08UlWepfUDs"}},{"cell_type":"code","source":["from sklearn.ensemble import HistGradientBoostingClassifier\n","hgb = HistGradientBoostingClassifier(random_state=42)\n","scores = cross_validate(hgb, train_input, train_target, return_train_score=True)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W-7w0tACeX4l","executionInfo":{"status":"ok","timestamp":1658318939375,"user_tz":-540,"elapsed":7138,"user":{"displayName":"박민규","userId":"03566772411550627054"}},"outputId":"624e2454-4f36-4d86-94f9-d1d8c0069776"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9321723946453317 0.8801241948619236\n"]}]},{"cell_type":"code","source":["from sklearn.inspection import permutation_importance\n","hgb.fit(train_input, train_target)\n","result = permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state=42, n_jobs=-1)\n","print(result.importances_mean)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o01ZuO9cgsWx","executionInfo":{"status":"ok","timestamp":1658319295105,"user_tz":-540,"elapsed":3876,"user":{"displayName":"박민규","userId":"03566772411550627054"}},"outputId":"e062931b-33c2-478c-8165-d7cb846aac6f"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.08876275 0.23438522 0.08027708]\n"]}]},{"cell_type":"markdown","source":["**permutation_importance()** 함수가 반환하는 객체는 반복하여 얻은 **특성 중요도(importances), 평균(importances_mean), 표준 편차(importances_std)**를 담고 있다. 평균을 출력해 보면 랜덤 포레스트와 비슷한 비율임을 알 수 있다. 이번엔 테스트 세트에서 특성 중요도를 계산해 보겠다."],"metadata":{"id":"Gy_W3pXfjATc"}},{"cell_type":"code","source":["result = permutation_importance(hgb, test_input, test_target, n_repeats=10, n_jobs=-1, random_state=42)\n","print(result.importances_mean)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EnT400bth-En","executionInfo":{"status":"ok","timestamp":1658319739023,"user_tz":-540,"elapsed":1195,"user":{"displayName":"박민규","userId":"03566772411550627054"}},"outputId":"b50553d7-e34f-4b0f-c945-ce29b15d2eaa"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.05969231 0.20238462 0.049     ]\n"]}]},{"cell_type":"code","source":["hgb.score(test_input, test_target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6rtjQXp0jsAc","executionInfo":{"status":"ok","timestamp":1658319773187,"user_tz":-540,"elapsed":271,"user":{"displayName":"박민규","userId":"03566772411550627054"}},"outputId":"188f242c-e80f-46bb-da35-5ba167129ab5"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8723076923076923"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["사이킷런 말고도 히스토그램 기반 그레이디언트 부스팅 알고리즘을 구현한 알고리즘이 여럿 있다."],"metadata":{"id":"lbQGQoRIj7jA"}},{"cell_type":"code","source":["from xgboost import XGBClassifier\n","xgb = XGBClassifier(tree_method='hist', random_state=42)\n","scores = cross_validate(xgb, train_input, train_target, return_train_score=True)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WIHCMMejj5i4","executionInfo":{"status":"ok","timestamp":1658319950492,"user_tz":-540,"elapsed":1289,"user":{"displayName":"박민규","userId":"03566772411550627054"}},"outputId":"25d35e18-073d-433f-a006-e2164f5f8876"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8824322471423747 0.8726214185237284\n"]}]},{"cell_type":"code","source":["from lightgbm import LGBMClassifier\n","lgb = LGBMClassifier(random_state=42)\n","scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BACDgTBrkkk3","executionInfo":{"status":"ok","timestamp":1658320136596,"user_tz":-540,"elapsed":3359,"user":{"displayName":"박민규","userId":"03566772411550627054"}},"outputId":"c509dcd2-d86d-42f5-bf2d-b3f996da19b8"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9338079582727165 0.8789710890649293\n"]}]}]}